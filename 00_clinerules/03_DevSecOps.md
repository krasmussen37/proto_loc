# DevSecOps Guidelines (proto_loc Analytics Platform)

## Overview

- **Clean Repository Structure:** Keep the top-level directory tidy. Aside from designated folders, the only file at the root should be **`README.md`**. Organize all content into the prescribed directories (e.g. `01_source_data/`, `02_duck_db/`, `03_dagster/`, etc.) and avoid adding loose files. Update the `README.md` with each significant change or commit so it always reflects the current state of the project. Maintaining an up-to-date README ensures that others (and automation tools) understand the repository’s latest structure and usage instructions.
    
- **Directory Layout:** Adhere to the defined project layout. The repository is structured into numbered directories for each component of the analytics-to-AI stack. For example:
    
    ```plaintext
    /                # Root of the repository
    ├── 00_clinerules/     # Contribution and rules (this document and similar)
    ├── 01_source_data/    # Raw source data files (ingested inputs)
    ├── 02_duck_db/        # DuckDB databases
    │   ├── 01_raw/        #   Raw DuckDB (ingestion layer)
    │   ├── 02_dev/        #   Development DuckDB (staging & marts for dev)
    │   └── 03_prod/       #   Production DuckDB (staging & marts for prod)
    ├── 03_dagster/        # Dagster orchestrator configuration
    ├── 04_dbt/            # dbt transformation project files
    ├── 05_cube_dev/       # Cube (semantic layer / OLAP) development config
    ├── 06_superset/       # Superset BI tool configuration
    ├── 07_pandas_ai/      # Notebooks or scripts for Pandas AI (LLM integration)
    ├── other/             # Other supporting code
    │   ├── docker/        #   Docker-related files
    │   ├── python/        #   Utility Python scripts or modules
    │   └── scripts/       #   Misc scripts
    └── README.md          # Project readme (must be kept updated)
    ```
    
    Do not deviate from this structure. If new top-level components are needed, discuss and document them in the README. A consistent structure makes navigation and automation easier.
    
- **Naming Conventions:** Use consistent naming across all tools and files for clarity. We **recommend** snake_case for file names, folder names, database objects (tables, schemas, etc.), and code identifiers. Snake case (all lowercase with underscores) preserves word boundaries across different systems – for example, many SQL databases fold unquoted names to lowercase, so using `snake_case` avoids ambiguities with capitalization. This convention improves readability and cross-platform compatibility. (This is not a strict rule if a specific tool imposes another convention, but strive for consistency).
    
- **.gitignore Setup:** Create a robust **`.gitignore`** in the repository root to prevent committing sensitive or irrelevant files. At minimum, **exclude** environment/secret files, data files, and build artifacts:
    
    - Secrets and config files (e.g. `.env`, any files containing API keys or credentials).
        
    - Large or raw data files (e.g. contents of `01_source_data/` and any binary DuckDB files like `*.duckdb`).
        
    - System/IDE files (e.g. `.vscode/`, `.idea/`, OS-specific files like `.DS_Store`).
        
    - Python caches or compiled files (`__pycache__/`, `*.pyc`) and other autogenerated outputs.
        
    
    Not tracking these ensures the repo stays lean, secure, and avoids exposing secrets. You can use templates (such as GitHub’s gitignore templates or [gitignore.io](https://gitignore.io/)) to bootstrap common ignore patterns. **Verify** your ignore rules by running commands like `git status` or `git check-ignore -v <file>` to see if a file is being ignored and by which pattern. Adjust the patterns if you notice any important file showing up or any sensitive file not being ignored. (Tip: Never assume a private repo is safe for secrets – always exclude them; many incidents of leaked keys have occurred via committed .env files.)
    
- **Open-Source License:** After forking or initializing this repository, **add a LICENSE file** to define how others can use and contribute to the project. We recommend choosing a permissive license like **MIT** or **Apache 2.0**. The **MIT License** is short and very permissive – it allows others to use, modify, and even redistribute your code in closed-source projects, as long as they include attribution. The **Apache License 2.0** is also highly permissive and additionally provides explicit protections for contributors and users regarding patents. In practice, both licenses let your project be used freely while safeguarding you and your contributors. To add a license, create a `LICENSE` file in the root with the full text of the chosen license (you can find templates on choosealicense.com or GitHub’s new repository setup). Picking a license is crucial for open collaboration – _do this as one of the first steps after cloning/forking_.
    

## Phase 1: Platform Infrastructure Setup

This phase covers assembling the platform’s infrastructure before any data or pipelines are added. The goal is to set up all tools in a data-agnostic way, so the stack is ready to accept data and code in Phase 2.

- **Initialize DuckDB Databases:** Set up empty DuckDB databases for each environment. Under `02_duck_db/`, you should have three subfolders: `01_raw/`, `02_dev/`, and `03_prod/`. Each corresponds to a DuckDB instance:
    
    - **Raw DuckDB** – for ingesting raw source data (read-only to transformations).
        
    - **Dev DuckDB** – for development (staging and intermediate outputs during development).
        
    - **Prod DuckDB** – for production (staging and final outputs for end use).
        
    
    Each subfolder can contain the DuckDB database file (for example, `02_duck_db/01_raw/raw.duckdb`, and similarly `dev.duckdb`, `prod.duckdb` for the others) along with any needed initialization scripts. Ensure these databases exist (even if empty) so that connections can be made. You can create an empty DuckDB file by running DuckDB with a path (it will create the file if it doesn’t exist), or via code using DuckDB libraries. The subfolders help organize any additional files (e.g. DuckDB WAL or backups, if any) per environment.
    
- **Configure Orchestration and Transformation Tools:** Set up the configuration for **Dagster** (orchestrator) and **dbt** (transformation layer) to integrate with the above DuckDBs:
    
    - In `03_dagster/`: configure Dagster to know about your resources and jobs. For example, define a DuckDB resource/IO manager that points to the **dev** DuckDB file by default (and raw DuckDB if it needs direct access). This might involve editing Dagster’s `workspace.yaml` or `code_location` files to include your project, and using Dagster’s DuckDB resource (`DuckDBResource`) with the path to your DuckDB database. The orchestrator should be aware of how to connect to both the raw data (for ingestion) and the dev database (for transformations). No actual pipelines are defined yet, but the plumbing should be in place.
        
    - In `04_dbt/`: initialize a dbt project (if not already) and set up a **profiles.yml** for DuckDB. The profiles configuration should define at least two targets (e.g. `dev` and `prod`), each pointing to the respective DuckDB file or schema. For example, a `dev` target could use the file in `02_dev/` and a `prod` target uses the file in `03_prod/`. Ensure the profile name matches the dbt project and that you can run `dbt debug` successfully. This will enable seamless switching of dbt runs between dev and prod later. At this stage, you likely have no models in the dbt project aside from maybe a sample or the default `schema.yml` – that’s fine, the key is the configuration is ready.
        
- **Environment Variables (.env):** Prepare a `.env` file in the project root to hold configuration secrets and keys needed by various components. For instance, you might need API keys for LLM or AI services (OpenAI, Anthropic, Google Gemini, etc.) so that the Pandas AI or other AI integration can function. In the `.env`, include placeholder entries such as:
    
    ```dotenv
    OPENAI_API_KEY=your-openai-key-here
    ANTHROPIC_API_KEY=your-anthropic-key-here
    GEMINI_API_KEY=your-gemini-key-here
    ```
    
    and any other relevant environment-specific settings (database connection strings, etc., if needed). **Do not commit real secrets.** In fact, the `.env` file should be listed in `.gitignore` (see Overview) so it's never accidentally tracked. Instead, provide a **`.env.example`** file (or a clearly documented section in the README) that lists all required environment variables with dummy sample values and comments. This acts as documentation for anyone setting up the project, indicating which keys they need to obtain. For example, `.env.example` might contain `OPENAI_API_KEY=sk-XXXXXXXXXXXXXXXXXXXX:contentReference[oaicite:14]{index=14}` (with X’s or a placeholder) and a comment like `# OpenAI API key for AI-driven queries`. By following this practice, you keep sensitive data out of the repo while making it easy for a new developer or AI agent to configure their environment correctly.
    
- **Sample .env Configuration:** If not creating a separate `.env.example` file, at least include in the **README** a snippet or table of required environment variables and their descriptions. For example:
    
    |Variable|Description|
    |---|---|
    |`OPENAI_API_KEY`|OpenAI API key (for LLM services)|
    |`ANTHROPIC_API_KEY`|Anthropic API key (if needed)|
    |`GEMINI_API_KEY`|Google Gemini API key (if needed)|
    |`...`|... other keys ...|
    
    This way, anyone forking or using the repo knows how to set up their `.env`. Always double-check that no actual secret values are present in any committed file (including this document and examples).
    
- **Tool-Ready, Data-Agnostic State:** At the end of Phase 1, the repository should have **all infrastructure in place but no actual data or pipelines yet**. This means:
    
    - The DuckDB instances exist but contain no tables (just empty databases ready to be populated).
        
    - Dagster is configured and can run (e.g. `dagster dev` comes up with no errors), but there are no data pipelines (assets or jobs) defined yet.
        
    - dbt is set up (you can do `dbt debug` successfully) but there are no or only placeholder models. No transformations or sources are defined yet.
        
    - Other components like Cube, Superset, etc., have their scaffolding or config in place, but not pointed at any specific data. For example, Superset might be installed with a blank configuration, Cube may have a dev server setup with no cubes defined yet.
        
    
    **No source data or custom pipelines should be committed in this phase.** The platform is essentially a template – all the tools are wired together and ready to go, but we haven’t introduced any domain-specific content. This ensures the repository can be forked or reused for different projects without carrying irrelevant data. It also forces good separation: the base stack is independent of any particular dataset or workflow.
    
    _Rationale:_ By keeping Phase 1 data-agnostic, we ensure that any new project starts from a clean slate of infrastructure. It’s easier to troubleshoot platform issues (since any error is in the setup, not in data logic) and ensures that Phase 2 (where actual development occurs) will plug into a well-prepared environment.
    

## Phase 2: Platform Usage and Operations

Once the platform is set up, Phase 2 covers how to use it for development and operations. These guidelines ensure consistency in how data flows from sources to raw storage to transformed outputs, maintain a clear dev/prod separation, and establish best practices so that both human developers and AI agents can collaborate safely.

- **Raw Data Ingestion:** All **source data files** should be placed in the `01_source_data/` directory. This includes CSVs, JSON files, exports from external systems, etc. When you acquire new data (e.g. a dump from a CRM or ERP system), save it under an appropriate subfolder or with a clear name in `01_source_data/`. **Use Dagster to load these files into DuckDB**: set up Dagster assets or jobs that read from `01_source_data/` and write to the **Raw DuckDB** (the database file in `02_duck_db/01_raw/`). Each source file or dataset should become a table in the raw database. Automating this via Dagster ensures reproducibility – you might have a Dagster pipeline that, for example, watches the `01_source_data/` folder and imports any new files into DuckDB tables. The raw loading processes should be idempotent (re-running it should update/replace the raw tables as needed). By centralizing raw file ingestion in the orchestrator, you ensure that the raw layer is always in sync with the source files and that there’s a clear, version-controlled process for loading data.
    
- **Database Schema Standards:** Organize the databases into logical schemas to keep data segmented and manageable. Adhere to the following standard:
    
    - **Raw Database (01_raw)** – Use **one schema per data source**. For example, if you have multiple sources like a CRM system and an ERP database feeding data, create schemas named `crm` and `erp` inside the raw DuckDB. All raw tables from the CRM export go into the `crm` schema (e.g. `crm.customers`, `crm.transactions_raw`), and all raw tables from the ERP go into `erp` schema, etc. This way, the raw layer preserves the origin of each data easily. It also prevents naming collisions between sources. The raw schemas and table names can mirror the source's naming (perhaps cleaned to snake_case). Keeping raw data structured by source makes lineage clear: you know exactly where each table came from.
        
    - **Development & Production Databases (02_dev & 03_prod)** – Use two schemas in each of these environments:
        
        - **`stg` schema**: for **staging models** – intermediate tables or views that transform and clean raw data. These are typically one-to-one or many-to-one transformations of raw source tables, applying business logic like renaming columns, fixing data types, filtering, deduplicating, etc. Staging schemas essentially prepare data for the next layer. In development (dev DB), you’ll iteratively build and test these staging models. In production (prod DB), only tested/approved staging models should appear.
            
        - **`mart` schema**: for **data marts** (final models) – curated, business-ready tables that serve as the single source of truth for analytics or AI consumption. Marts are often fact or dimension tables, wide denormalized tables, or any output that would be directly queried by analysts, dashboards, or AI agents. These are the polished data products. In development, you will materialize and verify mart models; in production, you materialize them for end-users.
            
    
    Both the dev and prod DuckDB should have the same schema names (`stg` and `mart`) to represent the same logical structure in each environment. Keeping schema names consistent means your dbt project and queries can easily target dev vs prod by switching connection, without needing to change schema references. This standardized layering (raw/staging/mart) aligns with good data warehouse practices (akin to bronze/silver/gold layers in data lakes). It provides a clear separation of concerns: raw = untouched data, stg = cleaned data, mart = business data.
    
- **Tables vs. Views (Materialization Guidelines):** Different layers may use different table types or materializations:
    
    - **Raw layer** – All data in the raw DB should be stored as **physical tables** (not views). These are the imported data as-is. Since raw tables are usually direct loads from files, they should be fully materialized to preserve the exact source data. There’s no need for views in raw; the data isn’t derived via queries at this stage.
        
    - **Staging layer (`stg` schemas)** – These can be implemented as **views or ephemeral models** (if using dbt) by default. The idea is that staging transformations are lightweight and simply _reflect_ the raw data in a cleaned form. Using views for staging keeps them always up-to-date with the latest raw data without storing duplicate copies. Views are quick to build (since they just run a SELECT), though keep in mind they query the raw tables on the fly. In practice, it’s fine because staging queries are usually not too heavy, and if they are, you can decide to materialize particular ones. A common best practice is to use views in staging until performance dictates otherwise. If a staging model is expensive to compute repeatedly, you might materialize it as a table, but start with views. (In dbt, you can also use `ephemeral` models for staging, which are neither tables nor views, but inlined subqueries – useful for purely intermediate steps that don’t need to be individually accessible.)
        
    - **Marts layer (`mart` schemas)** – These should generally be **materialized tables** (or occasionally incremental tables) for performance and usability. Marts are what end-users or downstream applications query, so they should be optimized for fast reads. A table in DuckDB will query much faster than a view that joins multiple staging tables, especially on large data, because it’s precomputed. Views in the mart layer could be acceptable for very small/simple data, but usually we want the final outputs materialized. As a rule: _use views for development agility, but tables for production reliability_. In dbt terms, you might set most mart models to `materialized: table` (or incremental) so that when you run in prod, it builds tables. If a mart table is extremely large or slow to rebuild, consider `incremental` materialization which updates only changed parts, achieving the same query performance with faster build times. Remember that views are faster to build (no storage, just logic) but slower to query, whereas tables take time to build but query quickly – balance these trade-offs per your needs. In summary: **staging = default to views**, **mart = default to tables** for the best of both worlds.
        
    
    _Note:_ Using tables for marts also allows you to add indexes or perform further optimization if needed (DuckDB will automatically optimize columns for storage). For staging views that are layered (view on view), be mindful of performance – many layers of views can degrade query speed. If you notice that, it may be time to materialize an intermediate result. The guiding principle is to only materialize (table) what you need to speed up, and keep everything else as views to reduce maintenance and duplication.
    
- **Development vs. Production Environment:** **All development and testing must happen in the dev environment (DuckDB `02_dev`).** The production database (`03_prod`) should only be updated through a deliberate promotion or release process by a human operator. In practice, this means:
    
    - When you (or an AI agent) are building new dbt models or writing Dagster pipelines, configure them to write to the **dev** database. For example, ensure the dbt target is `dev` (so it writes to `02_dev`), and if Dagster jobs directly write to DuckDB, they point to the dev DuckDB file or schemas.
        
    - Run all transformations and experiments on dev. Validate the results in dev (and possibly use Superset or other tools to point at dev for testing dashboards).
        
    - **Never** automatically run destructive operations on the prod database. The prod DB should remain pristine, only reflecting officially released code and ingested raw data.
        
    - When a set of changes is ready to go live, a human (developer or data engineer) should review and then execute the promotion: e.g. run `dbt` with the `prod` target to materialize models in the prod DB, or use a controlled Dagster job to copy data from dev to prod. This manual step is critical for oversight – it ensures that nothing reaches production without human approval.
        
    
    By enforcing this rule, we prevent scenarios where an AI agent or script might inadvertently corrupt production data or introduce unvalidated changes. Production is the environment consumed by end-users (and possibly by critical AI analysis), so it must be stable and trustworthy. Use permissions or separate credentials to reinforce this: the credentials that automated processes (or AI) use should **not** even have write access to the prod DuckDB. For example, you might mount the prod DuckDB file in read-only mode for AI agents, or simply never configure them with the prod path.
    
- **Promotion from Dev to Prod (dbt and others):** To make moving changes from dev to prod easier and less error-prone, configure your tools to handle environment switching gracefully:
    
    - In **dbt**, utilize **targets and variables** so the same code can run on dev or prod. As mentioned, define a `dev` target (default for development work) and a `prod` target in your profiles.yml. The only difference between them might be the schema or database name (or DuckDB file path). For instance, `dev` could use a schema prefix or a different DuckDB file, and `prod` uses the prod file. This way, promoting is as simple as changing the target and running `dbt run` again – no code changes. All references in your models should use `{{ ref('model_name') }}` or source definitions, so they automatically pick up the correct environment’s tables. Do **not** hard-code schema or database names in SQL; instead, use dbt’s config or `source()`/`ref()` semantics to remain environment-agnostic.
        
    - You can also use **dbt variables or Jinja logic** for environment-specific behavior. For example, you might want to limit data size in dev to speed up iteration. You can do this by checking the environment: _e.g._ in a model SQL, add a filter like `{% if target.name == 'dev' %} ...date >= last 3 days... {% endif %}` to only process a subset when in dev mode. This keeps dev runs fast while prod runs still process all data. Similarly, if using **Cube** or **Superset**, have separate dev vs prod configurations (like two Superset databases, one pointing to dev DuckDB and one to prod DuckDB). The idea is to be able to switch any component from dev to prod without confusion.
        
    - Write documentation or scripts for the promotion process. For instance, a README section could outline: “To promote changes to production: run `dbt run --target prod && dbt test --target prod`, then in Superset switch the data source to the prod database for dashboards.” If using Dagster, you might have a dedicated job to copy dev mart tables into prod or to validate differences. Make promotion a conscious, well-documented step.
        
    
    By using configuration to separate dev/prod, you **ensure reproducibility**: the prod environment can be rebuilt from the same code that runs in dev. This setup also enables continuous integration checks – e.g., you could automate that any PR runs `dbt build` on a temporary schema (simulating prod) to ensure everything works before merging.
    
- **AI Agents and Production Safety:** If AI agents (automated coding assistants or AI-driven data explorers) are contributing to this project, they must be sandboxed to the dev environment. **AI agents should never write to production**. This is both a safety and a governance measure. For example:
    
    - If you use an AI tool to automatically generate dbt models or queries, configure it to run against the dev database only. Review its output before manual promotion.
        
    - If you have a chatbot that can query the data (using PandasAI or SQL generation), give it access only to the **dev** or a query database, or ensure it has read-only access to prod. It should not have credentials that allow deleting or modifying prod data.
        
    - Commits or pull requests created by AI should be reviewed by human maintainers before being merged and deployed to prod. In other words, maintain a **“human-in-the-loop”** for any production change.
        
    
    The reason is straightforward: AI can be extremely helpful in development (suggesting code, finding insights) but it does not understand context or consequences like a human. It might make a destructive query or a misinformed transformation. By keeping AI on the dev side, any mistakes it makes will be caught in testing. We treat the prod environment as a sacred space where only trusted, verified changes go.
    
- **Monitoring and Logging:** (Operational best practice) Ensure that as you start running pipelines, you monitor their output. Dagster, for instance, provides logs for each run – keep an eye on these for failures in data ingestion or transformation. Implement alerting or at least manual checks for data quality (dbt tests can be used here). This is especially important once users or AI are relying on the data.
    
- **Continuous Improvement:** As development proceeds, update the **`README.md`** and this **DevSecOps guide** if needed. For example, if you add a new component (say, an `08_ml_models/` directory for machine learning outputs) or a new environment variable, document it. Treat documentation as part of the code – it should evolve with the project. This makes it easier for new contributors (human or AI) to get up to speed and follow the rules.
    

By following these Phase 2 guidelines, the data pipeline will remain organized and robust. All raw data is traceable to its source, transformations are structured and environment-aware, and production remains stable and secured. Developers can iterate quickly in the dev environment, and AI assistants can be utilized effectively without endangering production integrity. The end result is an analytics-to-AI stack that is maintainable, auditable, and ready to deliver insights safely.